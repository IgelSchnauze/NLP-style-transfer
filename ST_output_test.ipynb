{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e8vkfZSUsZ9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:33.970361Z",
     "start_time": "2022-05-10T18:30:33.967367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2599,
     "status": "ok",
     "timestamp": 1651063728987,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "e8vkfZSUsZ9b",
    "outputId": "7948a401-2f25-4a93-8e2e-6763856e3ff4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "gzBhWOnUDOXL",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:33.986029Z",
     "start_time": "2022-05-10T18:30:33.972417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6765,
     "status": "ok",
     "timestamp": 1651063735748,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "gzBhWOnUDOXL",
    "outputId": "62d90d2c-fab6-4b43-df5d-6a8fc99de710",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ab9ca873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.001468Z",
     "start_time": "2022-05-10T18:30:33.987919Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1651063735749,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "ab9ca873"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import youtokentome as yttm\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b6018908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.016622Z",
     "start_time": "2022-05-10T18:30:34.002346Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1651063735750,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "b6018908"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f92d980d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.032148Z",
     "start_time": "2022-05-10T18:30:34.017647Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1651063735750,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "f92d980d"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3089450a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.095017Z",
     "start_time": "2022-05-10T18:30:34.034069Z"
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1651063736348,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "3089450a"
   },
   "outputs": [],
   "source": [
    "token_model = yttm.BPE(model='models/100k_voc20k_all_embed_yttm.model', n_threads=-1)\n",
    "# token_model = yttm.BPE(model='/content/drive/MyDrive/Colab Notebooks/NLP_poems/models/100k_voc20k_all_embed_yttm.model', n_threads=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ca3d5413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.110736Z",
     "start_time": "2022-05-10T18:30:34.095945Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651063736349,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "ca3d5413"
   },
   "outputs": [],
   "source": [
    "class styleDataset(Dataset):\n",
    "    def __init__(self, data_list_of_list):\n",
    "        self.data = data_list_of_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def cut_length(self, limit):\n",
    "        self.data = [text if len(text)<limit else text[:limit] for text in self.data]\n",
    "\n",
    "    def distribute_data(self, share_tr, share_val):  # examle = (0.8, 0.1)\n",
    "        rand_i = np.random.permutation(len(self.data))\n",
    "        n1 = int(len(self.data) * share_tr)\n",
    "        n2 = int(len(self.data) * share_val)\n",
    "        return [self.data[i] for i in rand_i[0: n1]], \\\n",
    "               [self.data[i] for i in rand_i[n1: n1 + n2]], \\\n",
    "               [self.data[i] for i in rand_i[n1 + n2:]]\n",
    "    \n",
    "def padding_poem(batch):\n",
    "    pad_id = token_model.subword_to_id('<PAD>')\n",
    "    \n",
    "    batch = [torch.tensor(x) for x in batch]\n",
    "    batch = pad_sequence(batch, batch_first=False, padding_value=pad_id)  # batch_first=True -> [batch size, len]\n",
    "    return batch, 1  # return style\n",
    "    # return batch.to(device)\n",
    "\n",
    "def padding_news(batch):\n",
    "    pad_id = token_model.subword_to_id('<PAD>')\n",
    "    \n",
    "    batch = [torch.tensor(x) for x in batch]\n",
    "    batch = pad_sequence(batch, batch_first=False, padding_value=pad_id)\n",
    "    return batch, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "04f0c0b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.126007Z",
     "start_time": "2022-05-10T18:30:34.111668Z"
    },
    "code_folding": [
     0
    ],
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651063736349,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "04f0c0b2"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, style_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.style_dim = style_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) # 1 - size of dict emb, 2 - size of emb vec\n",
    "        self.style_embedding = nn.Embedding(2, style_dim) # 2, because 0-new, 1-poem\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim + style_dim, n_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_style):      \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        # style_token = torch.tensor([src_style, 1 - src_style]).to(device)\n",
    "        style_token = torch.tensor([src_style], device=device)\n",
    "\n",
    "        init_hidden = torch.cat((self.style_embedding(style_token), \n",
    "                                torch.zeros((1, self.hid_dim), device=device)), dim=1)\n",
    "        init_hidden = init_hidden.repeat(self.n_layers, src.shape[1], 1) \n",
    "        #init hidden = [n layers, batch size, style dim + hid_dim]\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded, init_hidden)\n",
    "        #outputs = [src len, batch size, style dim + hid dim]\n",
    "        #hidden = [n layers, batch size, style dim + hid dim]\n",
    "        \n",
    "        #pop style part\n",
    "        outputs = outputs[:,:, self.style_dim:]\n",
    "        hidden = hidden[:,:, self.style_dim:]\n",
    "        #outputs = [src len, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "\n",
    "        #outputs are always from the top hidden layer\n",
    "        #hidden - it will be first hid states in decoder\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "N7n7rHfqF90n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.141373Z",
     "start_time": "2022-05-10T18:30:34.127039Z"
    },
    "code_folding": [
     0
    ],
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651063736350,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "N7n7rHfqF90n"
   },
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hid_dim, style_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.style_dim = style_dim\n",
    "#         self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "#         self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        \n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "#         #hidden = [batch size, style dim + hid dim]\n",
    "#         #encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "#         batch_size = encoder_outputs.shape[1]\n",
    "#         src_len = encoder_outputs.shape[0]\n",
    "\n",
    "#         #from hidden pop style emb part \n",
    "#         hidden = hidden[:, self.style_dim:]\n",
    "#         #hidden = [batch size, hid dim + style emb]\n",
    "        \n",
    "#         #repeat decoder hidden state src_len times\n",
    "#         hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "#         #hidden = [batch size, src len, hid dim]\n",
    "#         #encoder_outputs = [batch size, src len, hid dim]\n",
    "        \n",
    "#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "#         #energy = [batch size, src len, hid dim]\n",
    "\n",
    "#         attention = self.v(energy).squeeze(2)\n",
    "#         #attention= [batch size, src len]\n",
    "        \n",
    "#         return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e524e3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.157516Z",
     "start_time": "2022-05-10T18:30:34.142260Z"
    },
    "code_folding": [
     0
    ],
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651063736350,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "e524e3a2"
   },
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_dim, emb_dim, hid_dim, style_dim, n_layers, dropout, attention):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.output_dim = output_dim  # the size of the vocabulary for the output/target.\n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.n_layers = n_layers\n",
    "#         self.style_dim = style_dim\n",
    "\n",
    "#         self.attention = attention\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim) \n",
    "#         self.style_embedding = nn.Embedding(2, style_dim)\n",
    "\n",
    "#         self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim + style_dim, n_layers) # dropout = dropout\n",
    "\n",
    "#         self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, input, hidden, encoder_outputs, trg_style):\n",
    "\n",
    "#         #input = [batch size]\n",
    "#         #hidden = [n layers, batch size, style dim + hid dim]\n",
    "#         #encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "#         input = input.unsqueeze(0) \n",
    "#         #input = [1, batch size]\n",
    "#         embedded = self.dropout(self.embedding(input))\n",
    "#         #embedded = [1, batch size, emb dim]\n",
    "\n",
    "#         if trg_style != -1: # make init hidden with style\n",
    "#             style_token = torch.tensor([trg_style], device=device)\n",
    "#             h_style = self.style_embedding(style_token)\n",
    "#             h_style = h_style.repeat(self.n_layers, hidden.shape[1], 1)\n",
    "#             #h_style = [n layers, batch size, style dim]\n",
    "\n",
    "#             hidden = torch.concat((h_style, hidden), dim=2)\n",
    "                \n",
    "#         last_hidden = hidden[-1]\n",
    "#         #last hidden = [batch size, style dim + hid dim]        \n",
    "#         a = self.attention(last_hidden, encoder_outputs)    \n",
    "#         #a = [batch size, src len]\n",
    "#         a = a.unsqueeze(1)\n",
    "#         #a = [batch size, 1, src len]\n",
    "        \n",
    "#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "#         #encoder_outputs = [batch size, src len, hid dim]\n",
    "        \n",
    "#         weighted = torch.bmm(a, encoder_outputs)    \n",
    "#         #weighted = [batch size, 1, hid dim]\n",
    "#         weighted = weighted.permute(1, 0, 2)\n",
    "#         #weighted = [1, batch size, hid dim]\n",
    "        \n",
    "#         rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "#         #rnn_input = [1, batch size, hid dim + emb dim]\n",
    "\n",
    "#         output, hidden = self.rnn(rnn_input, hidden)\n",
    "#         #output = [1, batch size, style dim + hid dim]\n",
    "#         #hidden = [n layers, batch size, style dim + hid dim]\n",
    "        \n",
    "#         embedded = embedded.squeeze(0)\n",
    "#         output = output.squeeze(0)\n",
    "#         weighted = weighted.squeeze(0)\n",
    "\n",
    "#         #pop style part from output\n",
    "#         output_pred = output[:, self.style_dim:]\n",
    "        \n",
    "#         prediction = self.fc_out(torch.cat((output_pred, weighted, embedded), dim = 1))\n",
    "#         #prediction = [batch size, output dim]\n",
    "        \n",
    "#         return output, prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f0ea421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.172965Z",
     "start_time": "2022-05-10T18:30:34.158539Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#style emb\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim, style_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.style_dim = style_dim\n",
    "        self.attn = nn.Linear(hid_dim * 2 + style_dim, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, style dim + hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, style dim + hid dim]\n",
    "        #encoder_outputs = [batch size, src len, hid dim]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #energy = [batch size, src len, hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "31b3bf76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.188719Z",
     "start_time": "2022-05-10T18:30:34.173858Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, style_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim  # the size of the vocabulary for the output/target.\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.style_dim = style_dim\n",
    "\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim) \n",
    "        self.style_embedding = nn.Embedding(2, style_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim + style_dim, n_layers) # dropout = dropout\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim + style_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, trg_style):\n",
    "\n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers, batch size, style dim + hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0) \n",
    "        #input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        if trg_style != -1: # make init hidden with style\n",
    "            style_token = torch.tensor([trg_style], device=device)\n",
    "            h_style = self.style_embedding(style_token)\n",
    "            h_style = h_style.repeat(self.n_layers, hidden.shape[1], 1)\n",
    "            #h_style = [n layers, batch size, style dim]\n",
    "\n",
    "            hidden = torch.concat((h_style, hidden), dim=2)\n",
    "                \n",
    "        last_hidden = hidden[-1]\n",
    "        #last hidden = [batch size, style dim + hid dim]        \n",
    "        a = self.attention(last_hidden, encoder_outputs)    \n",
    "        #a = [batch size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)    \n",
    "        #weighted = [batch size, 1, hid dim]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, hid dim + emb dim]\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        #output = [1, batch size, style dim + hid dim]\n",
    "        #hidden = [n layers, batch size, style dim + hid dim]\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return output, prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c49c37e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.204349Z",
     "start_time": "2022-05-10T18:30:34.189605Z"
    },
    "code_folding": [
     0
    ],
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1651063736754,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "c49c37e0"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, src_style, trg_style, tf_ratio):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio=0.75 -> use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        hid_dim = self.decoder.hid_dim\n",
    "        style_dim = self.decoder.style_dim\n",
    "        \n",
    "        predictions = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        outputs = torch.zeros(trg_len, batch_size, hid_dim + style_dim).to(self.device)\n",
    "         \n",
    "        #encoder_outputs is all hidden states of the input sequence\n",
    "        #hidden is the final hidden states, passed through a linear layer\n",
    "        enc_outputs, hidden = self.encoder(src, src_style)\n",
    "            \n",
    "        #first input to the decoder is the <bos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token emb-g, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, prediction, hidden = self.decoder(input, hidden, enc_outputs, trg_style)\n",
    "            #output = [batch size, hid dim + style dim]\n",
    "            \n",
    "            trg_style = -1 # turn off making init hidden with style\n",
    "            \n",
    "            predictions[t] = prediction\n",
    "            outputs[t] = output #?!?!?!?!first zeros <bos> or what????????\n",
    "            \n",
    "            teacher_force = random.random() < tf_ratio\n",
    "            \n",
    "            #get the highest predicted token\n",
    "            top1 = prediction.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token, else use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return predictions, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "zM5Jog2U4msV",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.220464Z",
     "start_time": "2022-05-10T18:30:34.205238Z"
    },
    "code_folding": [],
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651063736755,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "zM5Jog2U4msV"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, n_filters, filter_sizes, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim) # output_dim - длина словаря\n",
    "\n",
    "        # in_cnannels: In actual images this is usually 3 (one channel for each of the red, blue and green channels),\n",
    "        #             when using text we only have a single channel, the text itself\n",
    "        # out_channels: the number of filters\n",
    "        # kernel_size is the size of the filters = [n x emb_dim] where n is the size of the n-grams.\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=n_filters,\n",
    "                      kernel_size=(fs, emb_dim)) #fs - how token watch this filter\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, 2) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, gener_token_text):\n",
    "\n",
    "#         embedded = self.embedding(gener_token_text)\n",
    "        # embedded = [batch size, trg len, emb dim]\n",
    "\n",
    "        embedded = gener_token_text # embedded = [batch size, trg len, hid dim + style dim]\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch size, 1, trg len, hid dim]\n",
    "\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # element-wise relu(x) = max(0,x)\n",
    "        # conved_n = [batch size, n_filters, trg len - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "        predicts = self.fc(cat) # [x,y] - x bigger, then this isn't this style, y - it's this style\n",
    "        # predicts = [batch size, 2]\n",
    "        return predicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6c03553f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.452502Z",
     "start_time": "2022-05-10T18:30:34.222487Z"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1651063737328,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "6c03553f"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = token_model.vocab_size() \n",
    "OUTPUT_DIM = INPUT_DIM\n",
    "ENC_EMB_DIM = 128 \n",
    "DEC_EMB_DIM = 128\n",
    "STYLE_EMB_DIM = 256 # or try another \n",
    "HID_DIM = 256 \n",
    "N_LAYERS = 2 \n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "LABEL_DROPOUT = 0.3\n",
    "\n",
    "N_FILTERS = 128\n",
    "FILTER_SIZES = [1,3,4,5,8] #[2,3,4]\n",
    "\n",
    "attn = Attention(HID_DIM, STYLE_EMB_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, STYLE_EMB_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, STYLE_EMB_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "# discr_poem = Discriminator(OUTPUT_DIM, DEC_EMB_DIM, N_FILTERS, FILTER_SIZES, LABEL_DROPOUT).to(device) \n",
    "# discr_news = Discriminator(OUTPUT_DIM, DEC_EMB_DIM, N_FILTERS, FILTER_SIZES, LABEL_DROPOUT).to(device)\n",
    "discr_poem = Discriminator(OUTPUT_DIM, HID_DIM + STYLE_EMB_DIM, N_FILTERS, FILTER_SIZES, LABEL_DROPOUT).to(device) \n",
    "discr_news = Discriminator(OUTPUT_DIM, HID_DIM + STYLE_EMB_DIM, N_FILTERS, FILTER_SIZES, LABEL_DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a28baf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.467745Z",
     "start_time": "2022-05-10T18:30:34.453388Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651063737328,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "a28baf9d"
   },
   "outputs": [],
   "source": [
    "PAD_ID = token_model.subword_to_id('<PAD>')\n",
    "gener_criterion = nn.CrossEntropyLoss(ignore_index = PAD_ID)\n",
    "discr_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ZtjC3i8I47Fj",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:34.483506Z",
     "start_time": "2022-05-10T18:30:34.468773Z"
    },
    "code_folding": [],
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651063737329,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "ZtjC3i8I47Fj"
   },
   "outputs": [],
   "source": [
    "def test(model, discr_poem, discr_news, loader_p, loader_n, gener_criterion, discr_criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_loss_gen_rec = 0\n",
    "    epoch_loss_gen_stt = 0\n",
    "    epoch_loss_discrs = 0\n",
    "\n",
    "    acc_discr_poem_rec_right = 0 # poem discr get recon poem and told yes\n",
    "    acc_discr_news_rec_right = 0\n",
    "    acc_discr_poem_stt_is_st = 0 # poem discr get fake poem from news and told yes\n",
    "    acc_discr_news_stt_is_st = 0\n",
    "\n",
    "    out_list_news_recon = []\n",
    "    out_list_news_trans = []\n",
    "    out_list_poem_recon = []\n",
    "    out_list_poem_trans = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_p, batch_n in tqdm(zip(loader_p, loader_n), total=len(loader_p)):\n",
    "            for batch, src_style in (batch_p, batch_n):\n",
    "                batch = batch.to(device, non_blocking=True)\n",
    "                src = batch\n",
    "                trg = batch\n",
    "\n",
    "                predict_same, out_tokens_same = model(src, trg, src_style, src_style, 0) #turn off teacher forcing\n",
    "                predict_fake, out_tokens_fake = model(src, trg, src_style, 1 - src_style, 0) \n",
    "                #predict_same = [trg len, batch size, output dim]\n",
    "                #out_tokens_same = [trg len, batch size, hid dim]\n",
    "                \n",
    "                out_tokens_same_old = predict_same.argmax(2)\n",
    "                out_tokens_fake_old = predict_fake.argmax(2)\n",
    "                #out_tokens_same = [trg len, batch size]\n",
    "            \n",
    "                out_tokens_same_old = out_tokens_same_old.permute(1, 0) \n",
    "                out_tokens_fake_old = out_tokens_fake_old.permute(1, 0)\n",
    "                #out_tokens_same = [batch size, trg len]\n",
    "\n",
    "                if src_style == 0: # news\n",
    "                  out_list_news_recon.append(out_tokens_same_old)\n",
    "                  out_list_news_trans.append(out_tokens_fake_old)\n",
    "                if src_style == 1: # poem\n",
    "                  out_list_poem_recon.append(out_tokens_same_old)\n",
    "                  out_list_poem_trans.append(out_tokens_fake_old)\n",
    "                    \n",
    "#                 out_tokens_same = out_tokens_same_old\n",
    "#                 out_tokens_fake = out_tokens_fake_old\n",
    "                    \n",
    "                out_tokens_same = out_tokens_same.permute(1, 0, 2)\n",
    "                out_tokens_fake = out_tokens_fake.permute(1, 0, 2)\n",
    "                #out_tokens_same = [batch size, trg len, hid dim + style dim]\n",
    "\n",
    "                is_style = torch.ones((batch.shape[1]), device=device).long()\n",
    "                isnt_style = torch.zeros((batch.shape[1]), device=device).long()\n",
    "\n",
    "                if src_style == 0: # news\n",
    "                  discr_news_out_same = discr_news(out_tokens_same)\n",
    "                  discr_poem_out_fake = discr_poem(out_tokens_fake)\n",
    "\n",
    "                  loss_discr_news = discr_criterion(discr_news_out_same, is_style)\n",
    "                  loss_discr_poem = discr_criterion(discr_poem_out_fake, isnt_style) # find fault in style transfer \n",
    "\n",
    "                  loss_style_trans = discr_criterion(discr_poem_out_fake, is_style) # find success in style transfer\n",
    "\n",
    "                  acc_discr_news_rec_right += (discr_news_out_same.argmax(1) == 1).sum().item()\n",
    "                  acc_discr_poem_stt_is_st += (discr_poem_out_fake.argmax(1) == 1).sum().item()\n",
    "\n",
    "                if src_style == 1: # poem\n",
    "                  discr_news_out_fake = discr_news(out_tokens_fake)\n",
    "                  discr_poem_out_same = discr_poem(out_tokens_same)\n",
    "\n",
    "                  loss_discr_news = discr_criterion(discr_news_out_fake, isnt_style)\n",
    "                  loss_discr_poem = discr_criterion(discr_poem_out_same, is_style) \n",
    "\n",
    "                  loss_style_trans = discr_criterion(discr_news_out_fake, is_style)\n",
    "\n",
    "                  acc_discr_poem_rec_right += (discr_poem_out_same.argmax(1) == 1).sum().item()\n",
    "                  acc_discr_news_stt_is_st += (discr_news_out_fake.argmax(1) == 1).sum().item()\n",
    "                \n",
    "                #trg = [trg len, batch size]\n",
    "                #predict = [trg len, batch size, output dim]\n",
    "                predict_dim = predict_same.shape[-1]\n",
    "                predict_same = predict_same[1:].view(-1, predict_dim)\n",
    "                trg = trg[1:].view(-1)\n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #predict_same = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss_reconstr = gener_criterion(predict_same, trg)\n",
    "\n",
    "                loss_generator = loss_reconstr + loss_style_trans\n",
    "                loss_discr = loss_discr_poem + loss_discr_news\n",
    "\n",
    "                loss = loss_generator + loss_discr\n",
    "\n",
    "                epoch_loss += loss\n",
    "                epoch_loss_gen_rec += loss_reconstr\n",
    "                epoch_loss_gen_stt += loss_style_trans\n",
    "                epoch_loss_discrs += loss_discr\n",
    "\n",
    "    acc_discr_poem_rec = acc_discr_poem_rec_right / len(test_poem)\n",
    "    acc_discr_news_rec = acc_discr_news_rec_right / len(test_poem)\n",
    "\n",
    "    acc_discr_poem_stt = acc_discr_poem_stt_is_st / len(test_poem)\n",
    "    acc_discr_news_stt = acc_discr_news_stt_is_st / len(test_poem)\n",
    "        \n",
    "    return epoch_loss / len(loader_p), epoch_loss_gen_rec / len(loader_p), \\\n",
    "           epoch_loss_gen_stt / len(loader_p), epoch_loss_discrs / len(loader_p), \\\n",
    "           acc_discr_poem_rec, acc_discr_news_rec, \\\n",
    "           acc_discr_poem_stt, acc_discr_news_stt, \\\n",
    "           out_list_news_recon, out_list_news_trans, out_list_poem_recon, out_list_poem_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4v_oHkIGWHGB",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:36.917091Z",
     "start_time": "2022-05-10T18:30:36.583956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8259,
     "status": "ok",
     "timestamp": 1651063745584,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "4v_oHkIGWHGB",
    "outputId": "9e56773f-3b4f-406e-f8b0-67184a6de462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('ST_len20_dAdam005_genCyclefdf10_isnt3stt1rec1-af6e0-Copy1.5_discr14e_E6_checkp_last.pt')\n",
    "# checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP_poems/ST_15k_voc20k_len20_nf100_fs2,6_Lstt3_discrCyc0.1fdf1_checkp_last.pt', \n",
    "#                         map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "discr_poem.load_state_dict(checkpoint['discr_poem_state_dict'])\n",
    "discr_news.load_state_dict(checkpoint['discr_news_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6d0b297f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:39.894958Z",
     "start_time": "2022-05-10T18:30:39.888043Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1651063745585,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "6d0b297f"
   },
   "outputs": [],
   "source": [
    "with open('data/voc20klen20_test_poem.pickle', 'rb', ) as file:\n",
    "# with open('/content/drive/MyDrive/Colab Notebooks/NLP_poems/data/voc20klen20_test_poem.pickle', 'rb', ) as file:\n",
    "    test_poem = pickle.load(file)\n",
    "test_poem_loader = (\n",
    "    DataLoader(test_poem, batch_size=64, shuffle=False, num_workers=0, pin_memory=True, collate_fn=padding_poem)\n",
    ")\n",
    "\n",
    "with open('data/voc20klen20_test_news.pickle', 'rb', ) as file:\n",
    "# with open('/content/drive/MyDrive/Colab Notebooks/NLP_poems/data/voc20klen20_test_news.pickle', 'rb', ) as file:\n",
    "    test_news = pickle.load(file)\n",
    "test_news_loader = (\n",
    "    DataLoader(test_news, batch_size=64, shuffle=False, num_workers=0, pin_memory=True, collate_fn=padding_news)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6c4fe1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:30:44.608604Z",
     "start_time": "2022-05-10T18:30:40.356825Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "96726c6387c8421582ca95dbbf1725a4",
      "c0a80eed03654f4286a8253856e9dc23",
      "f5889def24a54bae8cf98e4417158359",
      "efbb9250cc044a00bae22fa663fb63f7",
      "fe518ed238524213a751a4ebb61e4169",
      "d912a8318c064df0a070fa1bacf30e0c",
      "1c3379e2c91543ceb9476aa2fbb47669",
      "ca66c185131c46c3abbe16180b525717",
      "149424d11ffc42c7bdcf0aca4d0a4b2f",
      "a0c45bb7820c44aebb5f73da32bf2188",
      "65502f417cd44677b5b7e3f3f095dee4"
     ]
    },
    "executionInfo": {
     "elapsed": 117415,
     "status": "ok",
     "timestamp": 1651063869215,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "6c4fe1ca",
    "outputId": "b45fdad9-e9d0-441a-fde3-bd97e0bfc4c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e757a4aa88d40f68c1cb995e72acbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test Loss: 19.078 | Acc recon poem: 0.01 | Acc recon news: 0.14 | Acc d_poem stt: 0.01 | Acc d_news stt: 0.12\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_loss_gen_rec, test_loss_gen_stt, test_loss_discrs, \\\n",
    "test_acc_recon_poem, test_acc_recon_news, test_acc_discr_poem_stt, test_acc_discr_news_stt, \\\n",
    "test_news_recon, test_news_trans, test_poem_recon, test_poem_trans = \\\n",
    "           test(model, discr_poem, discr_news, \n",
    "                test_poem_loader, test_news_loader, \n",
    "                gener_criterion, discr_criterion)\n",
    "\n",
    "print(f'\\t Test Loss: {test_loss:.3f} | Acc recon poem: {test_acc_recon_poem:.2f} | Acc recon news: {test_acc_recon_news:.2f} | ' +\n",
    "          f'Acc d_poem stt: {test_acc_discr_poem_stt:.2f} | Acc d_news stt: {test_acc_discr_news_stt:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bf83a",
   "metadata": {
    "id": "8d1bf83a"
   },
   "source": [
    "#### Get poems from test output and compare with orig test poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "n4266pQ29NrM",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:31:52.564598Z",
     "start_time": "2022-05-10T18:31:52.547020Z"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1651063930772,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "n4266pQ29NrM"
   },
   "outputs": [],
   "source": [
    "#[num batchs, batch len, trg len]\n",
    "gener_news_recon, gener_news_trans, gener_poem_recon, gener_poem_trans = [],[],[],[]\n",
    "#in one lines all data\n",
    "\n",
    "for i in range(len(test_news_recon)):\n",
    "    gener_news_recon.extend(test_news_recon[i]) \n",
    "    gener_news_trans.extend(test_news_trans[i])\n",
    "    gener_poem_recon.extend(test_poem_recon[i])\n",
    "    gener_poem_trans.extend(test_poem_trans[i])\n",
    "\n",
    "# with open('data/gener_news-poem_recon-trans_token.pickle', 'wb', ) as file:\n",
    "# # with open('/content/drive/MyDrive/Colab Notebooks/NLP_poems/data/voc20klen20_test_news.pickle', 'wb', ) as file:\n",
    "#     pickle.dump((gener_news_recon, gener_news_trans, gener_poem_recon, gener_poem_trans), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e97f6188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:31:55.218777Z",
     "start_time": "2022-05-10T18:31:52.846322Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD_ID = token_model.subword_to_id('<PAD>')\n",
    "BOS_ID = token_model.subword_to_id('<BOS>')\n",
    "EOS_ID = token_model.subword_to_id('<EOS>')\n",
    "\n",
    "gener_news_recon = token_model.decode(gener_news_recon, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])\n",
    "gener_news_trans = token_model.decode(gener_news_trans, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])\n",
    "gener_poem_recon = token_model.decode(gener_poem_recon, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])\n",
    "gener_poem_trans = token_model.decode(gener_poem_trans, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])\n",
    "\n",
    "orig_news_recon = token_model.decode(test_news, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])\n",
    "orig_poem_recon = token_model.decode(test_poem, ignore_ids=[PAD_ID, BOS_ID, EOS_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5ixdOx-jHWEu",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:31:55.234835Z",
     "start_time": "2022-05-10T18:31:55.219688Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1651063983795,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "5ixdOx-jHWEu",
    "outputId": "78bf4b3c-42cb-4f96-af54-ff9f71a6c75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сша великобритания и франция подтвердили свое участие в параде на красной площади в москве по случаю летия победы сообщил\n",
      "сша экономику и небольшие свое в в бельгий польши в воскресенье воскресенье в воскресенье воскресенье в воскресеньеен в\n",
      "сша безумно и очи свое в в одино | | в в небош | в в глазах прости\n",
      "\n",
      "акция твори добро приуроченная к великому посту началась в оренбурге в ее рамках по городу будет\n",
      "репети николай ссср россия к северму в четвергсб результате половину половину двух половину несколько хосе скончалсявшего\n",
      "обойй вдохновжаю к осень пурму гладь в жизни | песок | смотрю | галажему\n",
      "\n",
      "мародеры орудуют в гаитянском порт о пренсе который лежит в руинах\n",
      "полузащитник ред бом небольшие га гавриторияскиххаских матче финалабуласкихских аль пренских\n",
      "полузащитник ред меня сразу га гаври насля порт о прен всех утро |хахаитет\n",
      "\n",
      "сборная чехии одержит победу над россией в матче группового этапа хоккейного турнира ои заявил футболист московского\n",
      "сборная чехии полузащитника небольшие над счетомен в матче чемпионата испании чемпионата испании чемпионата национальной лиги кхл обыграв обыграв\n",
      "сборная чехии фелишал над над ще в матче групповогором дым групповогож наден на в дым\n",
      "\n",
      "пять человек погибли в понедельник в брянской области в дорожной аварии с участием грузовика и минивэ\n",
      "американский человек погибли в в воскресенье воскресеньенской воскресенье в многокварти скончался пройдет в воскресенье воскресенье в воскресенье воскресенье\n",
      "пять человек погибли в в вщлек | в в воскресенье все в в воскресенье угажки |\n",
      "\n",
      "следователи в четверг возбудили уголовное дело в отношении бывшего участкового уполномоченного милиции который на служебном автомобиле\n",
      "следователи в четверг возбудили уголовное в отношении бывшего глава ор ор бывшего проверку февраля во вторник бывшего проверку бывшего\n",
      "следователи в грамо играет в в в темной бывшего сирениряряппппряп\n",
      "\n",
      "голкипер итальянского футбольного клуба милан марко сторари будет выступать в сампдории на правах\n",
      "голкиперчные футбольного милан марко саха васиских сборной канады потерпе российскойццских матча\n",
      "разноитая футбольного | марко т грита шагуко сто стота лигуже\n",
      "\n",
      "хоккеисты сборной швеции со счетом переиграли команду германии на олимпиаде в ванкувере . голы в матче команд группы\n",
      "хоккеисты сборной швеции счетом переиграли на на своем в теннисном сборной со сибирь на чили чили чили сибирь\n",
      "хоккеисты сборной швеции счетом играет играет на своем | в | начномчномций на столе на на\n",
      "\n",
      "совет безопасности оон примет резолюцию об увеличении контингента военнослужащих и полицейских на гаити на тысячи человек во\n",
      "совет безопасности оон примет отправитьцию об обсерва на мэра страны и тран безопасности оон и и евро\n",
      "совет безопасности оон приметжаю первый первый | | и и не не и не руга и и не\n",
      "\n",
      "первый в иркутской области поезд консультативно диагностического центра академик федор углов отправится в рабо\n",
      "первый в иркутской областитонской десяти десяти академиг в ярмар кульном матче серб лиги ярмар ярмар\n",
      "первый в иркутской областито бродбаракгг шелерен кутерен куренен ку\n",
      "\n",
      "арбитражный суд москвы в пятницу продолжит после перерыва заседание по иску банка уралсиб к структурам корпорации\n",
      "арбитражный суд москвы в продолжит посленее вр суд марта май марта лаской марта марта марта марта\n",
      "арбитражный суд москвы в с деньнеенее по тобой янвапп и снег арбитражный голову вернулась |\n",
      "\n",
      "житель подмосковного жуковского в четверг в ходе ссоры ранил из травматического пистолета бизнесмена из брянска сообщил\n",
      "житель подмосковногоруковского в четверг в москвы из пистолета в пистолета двухэта пистолета травматического двух ставрополь пистолета\n",
      "житель подмосковногорулок в в в | наших сказа | в в в | гонит двух в в\n",
      "\n",
      "два точных удара чилийского форварда умберто суазо принесли сарагосе победу со\n",
      "два то ихуского форвардаскихжогобер чкуских вагого ч чских\n",
      "два то и хочетнкискихнки | спой спотогоха ггоготого\n",
      "\n",
      "девятый арбитражный апелляционный суд в четверг постановил перейти к рассмотрению в закрытом режиме апелляционной жалобы межрегиональ\n",
      "девятый арбитражный апелляционный в четверг четверг степени объектов объектов в четверг в среду бельгий в среду среду на\n",
      "девятый арбитражный апелляционный суд вкой я хочу я смотрю ввали вкой я хочу я смотрю\n",
      "\n",
      "премьер министр турции тайип эрдоган который находится в испании с официальным визитом стал миш\n",
      "премьер министр турции тайпп эрдоп эр который в турции в воскресенье керлин в воскресенье\n",
      "премьер министр турции тайпппппп тех впш виипп\n",
      "\n",
      "следствие по делу об убийстве адвоката станислава маркелова и журналистки анастасии бабуровой измени\n",
      "следствие по делу обчнуювые сибирьвке по керлин осчи национальной администрации ивановнской области\n",
      "следствие посудасуренвыевые шелкоке и иренренренренке игурен\n",
      "\n",
      "двоюродный брат саддама хусейна али хасан аль маджид по\n",
      "уфимю ввс аль каидыдамама аль альс аль ха аль аль аль ха аль аль\n",
      "двою беспонки саддадада думама потер потер потер слова | потер слова потер\n",
      "\n",
      "казанский рубин вышел в финала футбольного кубка чемпионов содружества стран снг и балтии который проходит в\n",
      "казанский рубин вышел в матче футбольного клуба чемпионата африканских чемпионата европы европы новосибир чемпионата национальной стран мира\n",
      "казанский рубин вышел в ду перекрестне просьром лиги стран странжж футбольного клуба перекре\n",
      "\n",
      "следственные органы следственного комитета по москве в пятницу предъявили обвинение н ачальнику следственного управления увд\n",
      "следственные органы следственного комитета москве в турции обвинитель органы следственного бывшего н следственного следственногонской проверку города следственного следственного\n",
      "следственные органытор по пояс в кап потерь втор по в в силу | | | в в\n",
      "\n",
      "военнослужащие международных сил содействия безопасности в афганистане в субботу застрелили подозрительного афганца который копал землю\n",
      "верховный международных сил содействия безопасности в европе европе фонд международных безопасности в европе воскресенье латвии сил европе в европе\n",
      "звудем сил содействиячто в в | играет мок его дней вщ вп | в европе\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "  print(orig_news_recon[i])\n",
    "  print(gener_news_recon[i])\n",
    "  print(gener_news_trans[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "r5RF3SUBJiEr",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:41:01.559518Z",
     "start_time": "2022-05-10T18:41:01.551435Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1651064042534,
     "user": {
      "displayName": "Вика",
      "userId": "07891139911062556455"
     },
     "user_tz": -180
    },
    "id": "r5RF3SUBJiEr",
    "outputId": "669a4b45-1e63-4701-de91-399ad15b596d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "катастрофа пожар потоп | но зато не прошла же мимо . | я набрасываюсь да\n",
      "букфалявого | но но не | | | . | | по | | | .\n",
      "мин академиля вызванного но семью мнение олимпиаде олимпиаде по футболу оон оон по футболу оон по футболу\n",
      "\n",
      "целую нежно я волчицу | она ж мурлычет мне в ответ | быть может все\n",
      "словами нежно я волчи | | ну тобой тобой нежно те | | долго |чи | долго\n",
      "обществензовая волчичи за фестиваль и обоснова майора централь марта мартачичи нового нового лекарства\n",
      "\n",
      "а через два часа на поезд | как я в качающий вагон | всегда по жизни пьяный\n",
      "а через два часа дома | | каксойск на ладони | | как будто как | стоят\n",
      "а ваз два предоставить дома дома свердловской как соотечественскск на дома мчруск на острова\n",
      "\n",
      "с порога пыхнешь сигаретой | войдешь без приглашенья в дом | и скажешь что\n",
      "с страсть пых |той | | | иросне | | | | ирос |\n",
      "с останов пластих погода пиро пива в европе и двухне и общежити с на общежити и с\n",
      "\n",
      "забавная крыска | жила в нашем доме | забавная крыска . | боялась ее\n",
      "обойвная кры | ж в в | зап | идио салила сано лашкошко\n",
      "юноше шведскав ж в вляется в турец погода сасоской провинции осское парк знаменитого\n",
      "\n",
      "иные любят безответно | в мученьях жизнь свою сжигая . | иным любовь лишь\n",
      "какиенит съе | в му | | | слеком | |роро | | кап |\n",
      "репетистноентно в муроперерабаты командирагоности общежитисуго общежитии общежитиенего\n",
      "\n",
      "женщина прекрасней розы | и восторженней мимозы | лучше всех на свете лилий | и\n",
      "ты говорила розы и зерка улыбки снегом и тихо тихо тешку | улыбки улыбки улыбки улыбки улыбки улыбки\n",
      "всемирно доволь розы и транспортной солоской станции метро севиль май столкнулся в воскресенье столкнулся в воскресенье столкнулся\n",
      "\n",
      "я полюбил холодные цвета | пронзительного питерского неба | в твоих глазах . | оско\n",
      "я полюбил . |знки пненене | |зз | |из\n",
      "российская австралийбил лекарства лекарстваз пснеских чемпионата хаджи в керлин во главе во\n",
      "\n",
      "все моющие средства меж собой | затеяли спор важный деловой | кто же из них воисти\n",
      "все моющие осеннись | | | | эх | | | | | | | |гу\n",
      "всеваниедержкилянение в европе на территории провинции ва территории на несколькование этого несколько несколько миллиона\n",
      "\n",
      "воздух от которого веет одиночеством | между зимой и весной . | полутона полуэмо\n",
      "воздух от тех вежки | | | между и вечер |ся огнем | и отшель под\n",
      "выставка от которого вевиви под лави междуской на нася столкнулсянского столкнулся столкнулсяскойской\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "  print(orig_poem_recon[i])\n",
    "  print(gener_poem_recon[i])\n",
    "  print(gener_poem_trans[i])\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ST_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (nlp_vik)",
   "language": "python",
   "name": "nlp_vik"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "149424d11ffc42c7bdcf0aca4d0a4b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c3379e2c91543ceb9476aa2fbb47669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65502f417cd44677b5b7e3f3f095dee4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96726c6387c8421582ca95dbbf1725a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0a80eed03654f4286a8253856e9dc23",
       "IPY_MODEL_f5889def24a54bae8cf98e4417158359",
       "IPY_MODEL_efbb9250cc044a00bae22fa663fb63f7"
      ],
      "layout": "IPY_MODEL_fe518ed238524213a751a4ebb61e4169"
     }
    },
    "a0c45bb7820c44aebb5f73da32bf2188": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0a80eed03654f4286a8253856e9dc23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d912a8318c064df0a070fa1bacf30e0c",
      "placeholder": "​",
      "style": "IPY_MODEL_1c3379e2c91543ceb9476aa2fbb47669",
      "value": "100%"
     }
    },
    "ca66c185131c46c3abbe16180b525717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d912a8318c064df0a070fa1bacf30e0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efbb9250cc044a00bae22fa663fb63f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0c45bb7820c44aebb5f73da32bf2188",
      "placeholder": "​",
      "style": "IPY_MODEL_65502f417cd44677b5b7e3f3f095dee4",
      "value": " 24/24 [01:57&lt;00:00,  4.29s/it]"
     }
    },
    "f5889def24a54bae8cf98e4417158359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca66c185131c46c3abbe16180b525717",
      "max": 24,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_149424d11ffc42c7bdcf0aca4d0a4b2f",
      "value": 24
     }
    },
    "fe518ed238524213a751a4ebb61e4169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
