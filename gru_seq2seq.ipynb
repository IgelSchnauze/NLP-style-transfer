{"cells":[{"cell_type":"code","source":["# import os\n","# os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""],"metadata":{"id":"R2u_OIa0-wx_"},"id":"R2u_OIa0-wx_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e8vkfZSUsZ9b","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:49.244369Z","start_time":"2022-03-10T19:30:49.229948Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1866,"status":"ok","timestamp":1649838177202,"user":{"displayName":"Вика","userId":"07891139911062556455"},"user_tz":-180},"id":"e8vkfZSUsZ9b","outputId":"b302dfee-774f-412a-f450-218623e65c91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"gzBhWOnUDOXL","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:49.248677Z","start_time":"2022-03-10T19:30:49.244369Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7519,"status":"ok","timestamp":1649838184717,"user":{"displayName":"Вика","userId":"07891139911062556455"},"user_tz":-180},"id":"gzBhWOnUDOXL","outputId":"a93d7a65-3904-4aaf-b91b-2e3b7fd769d7","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: youtokentome in /usr/local/lib/python3.7/dist-packages (1.0.6)\n","Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from youtokentome) (7.1.2)\n"]}],"source":["!pip install youtokentome"]},{"cell_type":"code","execution_count":null,"id":"ab9ca873","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.116815Z","start_time":"2022-03-10T19:30:49.248677Z"},"id":"ab9ca873"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import youtokentome as yttm\n","import numpy as np\n","\n","import random\n","import math\n","import time\n","import pickle\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"id":"3db6cce8","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.132750Z","start_time":"2022-03-10T19:30:50.116815Z"},"id":"3db6cce8"},"outputs":[],"source":["# %env CUDA_LAUNCH_BLOCKING=1"]},{"cell_type":"code","execution_count":null,"id":"b6018908","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.148802Z","start_time":"2022-03-10T19:30:50.133828Z"},"id":"b6018908"},"outputs":[],"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","# torch.use_deterministic_algorithms(True)"]},{"cell_type":"code","execution_count":null,"id":"f92d980d","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.196194Z","start_time":"2022-03-10T19:30:50.148802Z"},"id":"f92d980d"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"id":"3089450a","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.212010Z","start_time":"2022-03-10T19:30:50.196194Z"},"id":"3089450a"},"outputs":[],"source":["# token_model = yttm.BPE(model='try_poems_embed_yttm.model', n_threads=-1)\n","token_model = yttm.BPE(model='/content/drive/MyDrive/Colab Notebooks/NLP_poems/voc10k_poems_embed_yttm.model', n_threads=-1)"]},{"cell_type":"code","execution_count":null,"id":"4379886e","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:50.355459Z","start_time":"2022-03-10T19:30:50.213021Z"},"id":"4379886e"},"outputs":[],"source":["# with open('data/try_poems_preproc.pickle', 'rb', ) as file:\n","with open('/content/drive/MyDrive/Colab Notebooks/NLP_poems/data/voc10k_poems_preproc.pickle', 'rb', ) as file:\n","    preproc_data = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"id":"8793c637","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.578370Z","start_time":"2022-03-10T19:30:50.355459Z"},"id":"8793c637"},"outputs":[],"source":["token_data = token_model.encode(preproc_data, output_type=yttm.OutputType.ID, bos=True, eos=True)"]},{"cell_type":"code","execution_count":null,"id":"ddfeb192","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.594066Z","start_time":"2022-03-10T19:30:51.578370Z"},"id":"ddfeb192"},"outputs":[],"source":["# print(token_model.vocab())\n","# print(len(token_model.vocab()))\n","# print(preproc_data[2])\n","# print(token_data[2])\n","# print(len(preproc_data[1].split(' ')))\n","# print(len(token_data[1]))"]},{"cell_type":"code","execution_count":null,"id":"ca3d5413","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.631841Z","start_time":"2022-03-10T19:30:51.609867Z"},"id":"ca3d5413"},"outputs":[],"source":["class newDataset(Dataset):\n","    def __init__(self, data_list_of_list):\n","        self.data = data_list_of_list\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        return self.data[index]\n","\n","    def cut_length(self, limit):\n","#         middle_len = sum([len(text) for text in self.data])/len(self.data)\n","#         limit = int(middle_len)*2\n","        self.data = [text if len(text)<limit else text[:limit] for text in self.data]\n","\n","    def distribute_data(self, share_tr, share_val):  # examle = (0.8, 0.1)\n","        rand_i = np.random.permutation(len(self.data))\n","        n1 = int(len(self.data) * share_tr)\n","        n2 = int(len(self.data) * share_val)\n","        # return self.data[rand_i[0: n1]], self.data[rand_i[n1: n1 + n2]], self.data[rand_i[n1 + n2:]]\n","        return [self.data[i] for i in rand_i[0: n1]], \\\n","               [self.data[i] for i in rand_i[n1: n1 + n2]], \\\n","               [self.data[i] for i in rand_i[n1 + n2:]]\n","    \n","def padding(batch):\n","    pad_id = token_model.subword_to_id('<PAD>')\n","    \n","    batch = [torch.tensor(x) for x in batch]\n","    batch = pad_sequence(batch, batch_first=False, padding_value=pad_id)  # batch_first=True -> [batch size, len]\n","    return batch\n","    # return batch.to(device)\n","\n","def sort_padding(batch):\n","    pad_id = token_model.subword_to_id('<PAD>')\n","    \n","    batch.sort(key = lambda x : -len(x)) # first longest\n","\n","    batch_lens = [torch.tensor(len(x)) for x in batch]\n","    batch = [torch.tensor(x) for x in batch]\n","    batch = pad_sequence(batch, batch_first=False, padding_value=pad_id)  # batch_first=True -> [batch size, len]\n","    return batch, torch.tensor(batch_lens)"]},{"cell_type":"code","execution_count":null,"id":"753dfd9e","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.712181Z","start_time":"2022-03-10T19:30:51.665563Z"},"id":"753dfd9e"},"outputs":[],"source":["token_dataset = newDataset(token_data)\n","token_dataset.cut_length(40)\n","train_data, valid_data, test_data = token_dataset.distribute_data(0.8, 0.1)\n","train_loader, valid_loader, test_loader = \\\n","    DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0, pin_memory=True, collate_fn=sort_padding), \\\n","    DataLoader(valid_data, batch_size=128, shuffle=True, num_workers=0, pin_memory=True, collate_fn=sort_padding), \\\n","    DataLoader(test_data, batch_size=64, shuffle=False, num_workers=0, pin_memory=True, collate_fn=sort_padding)\n","\n","# with open('data/try_test_data_token.pickle', 'wb', ) as file:\n","# with open('/content/drive/MyDrive/Colab Notebooks/NLP_poems/data/voc10klen100_test_data_token.pickle', 'wb', ) as file:\n","#     pickle.dump(test_data, file)"]},{"cell_type":"code","execution_count":null,"id":"61b9cb35","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.728413Z","start_time":"2022-03-10T19:30:51.712181Z"},"id":"61b9cb35"},"outputs":[],"source":["# print(len(train_data))\n","# print(train_loader.batch_size)\n","# print(len(train_loader))\n","# for b in train_loader:\n","#   print(type(b[0]))\n","#   print(type(b[1]))\n","#   break"]},{"cell_type":"code","execution_count":null,"id":"04f0c0b2","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.744099Z","start_time":"2022-03-10T19:30:51.728413Z"},"code_folding":[0],"id":"04f0c0b2"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        \n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        \n","        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers) # dropout = dropout\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_nopad_len):      \n","        #src = [src len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(src))\n","        #embedded = [src len, batch size, emb dim]\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_nopad_len.to('cpu'))\n","                \n","        packed_outputs, hidden = self.rnn(packed_embedded)\n","        #packed_outputs is a packed sequence containing all hidden states\n","        #hidden is now from the final non-padded element in the batch\n","            \n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n","        #outputs is now a non-packed sequence\n","        \n","        #outputs = [src len, batch size, hid dim]\n","        #hidden = [n layers, batch size, hid dim]\n","        \n","        #outputs are always from the top hidden layer\n","        #hidden - it will be first hid states in decoder\n","        return outputs, hidden"]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, hid_dim):\n","        super().__init__()\n","        \n","        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n","        self.v = nn.Linear(hid_dim, 1, bias = False)\n","        \n","    def forward(self, hidden, encoder_outputs, mask):\n","        \n","        #hidden = [batch size, hid dim]\n","        #encoder_outputs = [src len, batch size, hid dim]\n","        #mask = [batch size, src len]  \n","        # example: sent=['hello', <pad>, <pad>], mask=[1,0,0]\n","        \n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","        \n","        #repeat decoder hidden state src_len times\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        #hidden = [batch size, src len, hid dim]\n","        #encoder_outputs = [batch size, src len, hid dim]\n","        \n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n","        #energy = [batch size, src len, hid dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","        attention = attention.masked_fill(mask == 0, -1e10)\n","        #attention= [batch size, src len]\n","        \n","        return F.softmax(attention, dim=1)"],"metadata":{"id":"QM2n9-leQojC"},"id":"QM2n9-leQojC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e524e3a2","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.759987Z","start_time":"2022-03-10T19:30:51.745272Z"},"code_folding":[0],"id":"e524e3a2"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n","        super().__init__()\n","        \n","        self.output_dim = output_dim  # the size of the vocabulary for the output/target.\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","\n","        self.attention = attention\n","        \n","        self.embedding = nn.Embedding(output_dim, emb_dim) # 1 - size of dict emb, 2 - size of emb vec\n","        \n","        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim, n_layers) # dropout = dropout\n","        \n","        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, encoder_outputs, mask):\n","\n","        #input = [batch size]\n","        #hidden = [n layers, batch size, hid dim]\n","        #encoder_outputs = [src len, batch size, hid dim]\n","        \n","        input = input.unsqueeze(0) \n","        #input = [1, batch size]\n","        embedded = self.dropout(self.embedding(input))\n","        #embedded = [1, batch size, emb dim]\n","                \n","        last_hidden = hidden[-1]\n","        #hidden = [batch size, hid dim]        \n","        a = self.attention(last_hidden, encoder_outputs, mask)    \n","        #a = [batch size, src len]\n","        a = a.unsqueeze(1)\n","        #a = [batch size, 1, src len]\n","        \n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        #encoder_outputs = [batch size, src len, hid dim]\n","        \n","        weighted = torch.bmm(a, encoder_outputs)    \n","        #weighted = [batch size, 1, hid dim]\n","        weighted = weighted.permute(1, 0, 2)\n","        #weighted = [1, batch size, enc hid dim]\n","        \n","        rnn_input = torch.cat((embedded, weighted), dim = 2)\n","        #rnn_input = [1, batch size, hid dim + emb dim]\n","        \n","        output, hidden = self.rnn(rnn_input, hidden)\n","        #output = [1, batch size, hid dim], seq len will always be 1 in the decoder\n","        #hidden = [n layers, batch size, hid dim]\n","        \n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","        \n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n","        #prediction = [batch size, output dim]\n","        \n","        return prediction, hidden"]},{"cell_type":"code","execution_count":null,"id":"c49c37e0","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:51.775757Z","start_time":"2022-03-10T19:30:51.759987Z"},"code_folding":[0],"id":"c49c37e0"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, src_pad_idx, device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.device = device\n","\n","    def create_mask(self, src):\n","        mask = (src != self.src_pad_idx).permute(1, 0)\n","        return mask\n","        \n","    def forward(self, src, src_nopad_len, trg, teacher_forcing_ratio = 0.2):\n","        \n","        #src = [src len, batch size]\n","        #src_nopad_len = [batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio=0.75 -> use ground-truth inputs 75% of the time\n","        \n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #encoder_outputs is all hidden states of the input sequence\n","        #hidden is the final hidden states, passed through a linear layer\n","        encoder_outputs, hidden = self.encoder(src, src_nopad_len)\n","        \n","        #first input to the decoder is the <bos> tokens\n","        input = trg[0,:]  \n","        mask = self.create_mask(src)\n","        #mask = [batch size, src len]\n","        \n","        for t in range(1, trg_len):\n","            \n","            #insert input token emb-g, previous hidden state and all encoder hidden states\n","            #receive output tensor (predictions) and new hidden state\n","            output, hidden = self.decoder(input, hidden, encoder_outputs, mask)\n","            \n","            outputs[t] = output\n","            \n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            #get the highest predicted token\n","            top1 = output.argmax(1) \n","            \n","            #if teacher forcing, use actual next token, else use predicted token\n","            input = trg[t] if teacher_force else top1\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":null,"id":"6c03553f","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.294314Z","start_time":"2022-03-10T19:30:51.775757Z"},"id":"6c03553f"},"outputs":[],"source":["INPUT_DIM = token_model.vocab_size() \n","OUTPUT_DIM = INPUT_DIM\n","ENC_EMB_DIM = 128 #128\n","DEC_EMB_DIM = 128 #128\n","HID_DIM = 256 #256\n","N_LAYERS = 2 \n","ENC_DROPOUT = 0.2\n","DEC_DROPOUT = 0.2\n","PAD_ID = token_model.subword_to_id('<PAD>')\n","\n","attn = Attention(HID_DIM)\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, PAD_ID, device).to(device)\n","# writer = SummaryWriter(log_dir='runs/Adam0001_Cycle005_dr02_e20_voc10k')\n","writer = SummaryWriter(log_dir='/content/drive/MyDrive/Colab Notebooks/NLP_poems/runs/AdamW01_C005_drEm2_voc10k_tf2_len40_AttPad')"]},{"cell_type":"code","execution_count":null,"id":"cb0355a5","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.310528Z","start_time":"2022-03-10T19:30:54.294314Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1649838199598,"user":{"displayName":"Вика","userId":"07891139911062556455"},"user_tz":-180},"id":"cb0355a5","outputId":"6a444ce4-013a-4b5c-f7da-5d5dda704027"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 10,680,592 trainable parameters\n"]}],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"id":"5082533d","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.326492Z","start_time":"2022-03-10T19:30:54.310528Z"},"id":"5082533d"},"outputs":[],"source":["N_EPOCHS = 20\n","CLIP = 1\n","\n","# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01) \n","\n","# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n","scheduler = optim.lr_scheduler.OneCycleLR(\n","    optimizer, max_lr = 0.005, total_steps = len(train_loader)*N_EPOCHS, cycle_momentum=True\n",")\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_ID)"]},{"cell_type":"code","execution_count":null,"id":"0611a641","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.358305Z","start_time":"2022-03-10T19:30:54.344168Z"},"code_folding":[0],"id":"0611a641"},"outputs":[],"source":["def train(model, loader, optimizer, scheduler, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(tqdm(loader)):\n","        batch_src = batch[0].to(device, non_blocking=True)\n","        batch_src_nopad_len = batch[1]\n","        src, src_nopad_len = batch_src, src_nopad_len\n","        trg = batch_src\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, src_nopad_len, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        loss.backward()\n","        \n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        scheduler.step() # for Cycle\n","        \n","        epoch_loss += loss.item()\n","        \n","#     scheduler.step() # for SGD\n","        \n","    return epoch_loss / len(loader)"]},{"cell_type":"code","execution_count":null,"id":"b8f7ec79","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.373513Z","start_time":"2022-03-10T19:30:54.358305Z"},"code_folding":[0],"id":"b8f7ec79"},"outputs":[],"source":["def evaluate(model, loader, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(tqdm(loader)):\n","            batch_src = batch[0].to(device, non_blocking=True)\n","            src, src_nopad_len = batch_src, batch[1]\n","            trg = batch_src\n","\n","            output = model(src, src_nopad_len, trg, 0) #turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(loader)"]},{"cell_type":"code","execution_count":null,"id":"36bb6787","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.389752Z","start_time":"2022-03-10T19:30:54.375237Z"},"id":"36bb6787"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"id":"07b0ed29","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.421392Z","start_time":"2022-03-10T19:30:54.405599Z"},"id":"07b0ed29"},"outputs":[],"source":["# torch.cuda.empty_cache()\n","# torch.cuda.memory_summary(device=None, abbreviated=False)  # give info about the alloaction of memory\n","\n","# torch.multiprocessing.set_start_method('spawn')  # for num_workers in Dataloader\n","# torch.cuda.device_count()"]},{"cell_type":"code","execution_count":null,"id":"5kNvOdBT46Qw","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:30:54.453146Z","start_time":"2022-03-10T19:30:54.437618Z"},"id":"5kNvOdBT46Qw"},"outputs":[],"source":["# checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/NLP_poems/b7e20n2_10k_checkpoint.pt')\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","# start_epoch = checkpoint['epoch']\n","# bv_loss = checkpoint['loss']"]},{"cell_type":"code","execution_count":null,"id":"55df031a","metadata":{"ExecuteTime":{"end_time":"2022-03-10T19:42:23.634706Z","start_time":"2022-03-10T19:30:56.116979Z"},"colab":{"base_uri":"https://localhost:8080/","height":377,"referenced_widgets":["2ad4d6ab3427473eb0edc132f201121a","f0b38b8d1db646ed9887763cce02d78b","47989c790d954c449be409fbfee96381","443f7fb2516340c4a873f5ee8f14aad3","7a9a6c30ffcd46e5a0b50402556cd992","c4e4adc202684f22a7292d05b08f913d","e1c3faa209f548f39a0f9e99417e9db7","ad6477c00934465a9d3ba9cc096426b7","7a4aef3ba70d43ca8e136dfa0d0fe9a1","cf97b92ba5b0485a81c83e5759423a79","a192b265b17847919bcd1684c8a3570b"]},"executionInfo":{"elapsed":250,"status":"error","timestamp":1649838261768,"user":{"displayName":"Вика","userId":"07891139911062556455"},"user_tz":-180},"id":"55df031a","outputId":"b79a1397-06bd-409f-e303-66e3a01d5418"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/112 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad4d6ab3427473eb0edc132f201121a"}},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-a9674ecb61a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-ffc6479a4a6c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, scheduler, criterion, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_nopad_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-a214342aad71>\u001b[0m in \u001b[0;36msort_padding\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch_first=True -> [batch size, len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"]}],"source":["bv_loss = float('inf')\n","best_valid_loss = bv_loss \n","\n","start_epoch = 0\n","for epoch in range(start_epoch, start_epoch + N_EPOCHS):\n","      \n","    start_time = time.time()\n","        \n","    train_loss = train(model, train_loader, optimizer, scheduler, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_loader, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n","    writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'loss': best_valid_loss\n","            # }, 'AdamW01_Cycle005_dr02_e20_checkp_best.pt')\n","            }, '/content/drive/MyDrive/Colab Notebooks/NLP_poems/AdamW01_C005_drEm2_voc10k_tf2_len40_AttPad_checkp_best.pt')\n","\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'loss': valid_loss\n","        # }, 'AdamW01_Cycle005_dr02_e20_checkp_last.pt')\n","            }, '/content/drive/MyDrive/Colab Notebooks/NLP_poems/AdamW01_C005_drEm2_voc10k_tf2_len40_AttPad_checkp_last.pt')\n","        \n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","    \n","writer.flush()\n","writer.close()"]},{"cell_type":"code","source":["writer.flush()\n","writer.close()"],"metadata":{"id":"yauVzUAb8n81"},"id":"yauVzUAb8n81","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f2d6a9e7","metadata":{"id":"f2d6a9e7"},"outputs":[],"source":["# %load_ext tensorboard\n","# %tensorboard --logdir mylogdir"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"gru_seq2seq.ipynb","provenance":[]},"kernelspec":{"display_name":"Python (nlp_vik)","language":"python","name":"nlp_vik"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"2ad4d6ab3427473eb0edc132f201121a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0b38b8d1db646ed9887763cce02d78b","IPY_MODEL_47989c790d954c449be409fbfee96381","IPY_MODEL_443f7fb2516340c4a873f5ee8f14aad3"],"layout":"IPY_MODEL_7a9a6c30ffcd46e5a0b50402556cd992"}},"f0b38b8d1db646ed9887763cce02d78b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4e4adc202684f22a7292d05b08f913d","placeholder":"​","style":"IPY_MODEL_e1c3faa209f548f39a0f9e99417e9db7","value":"  0%"}},"47989c790d954c449be409fbfee96381":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad6477c00934465a9d3ba9cc096426b7","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a4aef3ba70d43ca8e136dfa0d0fe9a1","value":0}},"443f7fb2516340c4a873f5ee8f14aad3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf97b92ba5b0485a81c83e5759423a79","placeholder":"​","style":"IPY_MODEL_a192b265b17847919bcd1684c8a3570b","value":" 0/112 [00:00&lt;?, ?it/s]"}},"7a9a6c30ffcd46e5a0b50402556cd992":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e4adc202684f22a7292d05b08f913d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1c3faa209f548f39a0f9e99417e9db7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad6477c00934465a9d3ba9cc096426b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a4aef3ba70d43ca8e136dfa0d0fe9a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf97b92ba5b0485a81c83e5759423a79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a192b265b17847919bcd1684c8a3570b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}